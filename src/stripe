/**
 * Bespin: reference implementations of "big data" algorithms
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

 package ca.uwaterloo.cs451.a1;



 import java.util.HashSet;
 import java.util.Set;
 import io.bespin.java.util.Tokenizer;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.conf.Configured;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.io.FloatWritable;
 import org.apache.hadoop.io.IntWritable;
 import org.apache.hadoop.mapreduce.Partitioner;
 import org.apache.hadoop.io.LongWritable;
 import org.apache.hadoop.io.Text;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.Mapper;
 import org.apache.hadoop.mapreduce.Reducer;
 import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
 import org.apache.hadoop.util.Tool;
 import org.apache.hadoop.util.ToolRunner;
 import org.apache.log4j.Logger;
 import org.apache.spark.sql.catalyst.expressions.Substring;
 import org.kohsuke.args4j.CmdLineException;
 import org.kohsuke.args4j.CmdLineParser;
 import org.kohsuke.args4j.Option;
 import org.kohsuke.args4j.ParserProperties;
 import org.omg.CORBA.WCharSeqHelper;
 
 import tl.lin.data.pair.PairOfStrings;
 import tl.lin.data.pair.PairOfWritables;
import tl.lin.data.map.HashMapWritable;
import tl.lin.data.pair.PairOfFloatInt;
 
 import java.io.*;
 import java.util.*;
 import java.net.*;
 import org.apache.hadoop.fs.*;
 import org.apache.hadoop.conf.*;
 import org.apache.hadoop.io.*;
 import org.apache.hadoop.mapred.*;
 import org.apache.hadoop.util.*;
 
 import java.io.IOException;
 import java.util.Iterator;
 
 
 public class StripesPMI extends Configured implements Tool {
   private static final Logger LOG = Logger.getLogger(StripesPMI.class);
     // Mapper: emits (token, 1) for every unique word occurrence.
    public static final class MapperOne extends Mapper<LongWritable, Text, Text, IntWritable> {
     // Reuse objects to save overhead of object creation.
    private static final IntWritable ONE = new IntWritable(1);
    private static final Text WORD = new Text();
    @Override
    public void map(LongWritable key, Text value, Context context)
        throws IOException, InterruptedException {
									//same as PairsPMI implementation
     context.getCounter("count", "totalLines").increment(1);
						List<String> tokens = Tokenizer.tokenize(value.toString());
      Set<String> seenWords = new HashSet<String>();
      int maxSize = Math.min(tokens.size(), 40);
      for (int i = 0; i < maxSize; i++)    {
							//WORD.set(tokens.get(i));
							//context.write(WORD, ONE);
							if (!seenWords.contains(tokens.get(i))) {//emit if we haven't seen it
									WORD.set(tokens.get(i));
									context.write(WORD, ONE); 
							}
							seenWords.add(tokens.get(i));
    		}
					}
   }
 
   //output stripes
   public static final class MapperTwo extends Mapper<LongWritable, Text, PairOfStrings, HashMapWritable<Text, PairOfFloatInt>> {
     // Reuse objects to save overhead of object creation.
     private static final IntWritable ONE = new IntWritable(1);
     private static final PairOfStrings PAIR = new PairOfStrings();
    
     
 
     @Override
     public void map(LongWritable key, Text value, Context context)
         throws IOException, InterruptedException {
       List<String> tokens = Tokenizer.tokenize(value.toString());
       Set<String> seenWords = new HashSet<String>();
       int maxSize = Math.min(tokens.size(), 40);
       for (int i = 0; i < maxSize; i++)    {
         if (!seenWords.contains(tokens.get(i))) {//add tokens to set if we havent seen it on the line yet
           for (String str: seenWords) {
             PAIR.set(str, tokens.get(i)); 
             context.write(PAIR, ONE);
           }          
           seenWords.add(tokens.get(i));
         }
       }
     }
   }
   
   private static final class MyCombiner extends
        Reducer<Text, FloatWritable, Text, FloatWritable> {
      private static final FloatWritable SUM = new FloatWritable();
  
      @Override
      public void reduce(Text key, Iterable<FloatWritable> values, Context context)
          throws IOException, InterruptedException {
        int sum = 0;
        Iterator<FloatWritable> iter = values.iterator();
        while (iter.hasNext()) {
          sum += iter.next().get();
        }
        SUM.set(sum);
        context.write(key, SUM);
      }
    }
 
   // Reducer: sums up all the counts of single word occurrences
   public static final class StripesPMIReducer extends Reducer<Text, IntWritable, Text, IntWritable> {
     // Reuse objects.
     private static final IntWritable SUM = new IntWritable();
     @Override
     public void reduce(Text key, Iterable<IntWritable> values, Context context)
         throws IOException, InterruptedException {
       // Sum up values.
       Iterator<IntWritable> iter = values.iterator();
       int sum = 0;
       while (iter.hasNext()) {
         sum += iter.next().get();
       }
       SUM.set(sum);
       context.write(key, SUM);
     }
   }
 
   //sums up pairs and calculates PMI using word counts from first map-reduce
   public static final class ReducerTwo extends Reducer<PairOfStrings, IntWritable, PairOfStrings, HashMapWritable<PairOfStrings, PairOfFloatInt>> {
     // Reuse objects.
     //math.log10 returns a double so we're just gonna use a double i guess
     private static PairOfFloatInt PAIR = new PairOfFloatInt();
     private static Map<String, Integer> wc = new HashMap<String, Integer>();
     private static double totalLines; 
     private static int threshold;
     protected void setup(Context context) throws IOException, InterruptedException {
       Configuration conf = context.getConfiguration();
       totalLines = conf.getDouble("lineCount", 2);
       threshold = conf.getInt("threshold", 1);
 
       //LOG.info("lineNo in reduce: " + totalLines);
       try{
         FileSystem fs = FileSystem.get(conf);
         FileStatus[] status = fs.listStatus(new Path("testdir"));  
         //LOG.info("before for" + status.length);
       for (int i=0;i<status.length;i++){
           //LOG.info("before buffered read");
           BufferedReader br=new BufferedReader(new InputStreamReader(fs.open(status[i].getPath())));
           //LOG.info("after buffered read");
           String line;
           line=br.readLine();
           while (line != null){
             //LOG.info("line: " + line);
             String[] tokens = line.split("\t");
             //LOG.info("token size: " + tokens.length);
             if (tokens.length == 2) {
               wc.put(tokens[0], Integer.parseInt(tokens[1]));
               //System.out.println("Tokens: " + tokens[0] + tokens[1]);
             }
            // 
             line = br.readLine();
           }
         }
       }catch(Exception e){
           System.out.println(e.getMessage());
       }
     }
     
     @Override
     public void reduce(PairOfStrings key, Iterable<IntWritable> values, Context context)
         throws IOException, InterruptedException {
       // Sum up values.
       //LOG.info("start of reduce");
       //LOG.info("after print");
 
       Iterator<IntWritable> iter = values.iterator();
       int sum = 0;
       double pX = wc.get(key.getLeftElement()) / totalLines;
       double pY = wc.get(key.getRightElement()) / totalLines;
       while (iter.hasNext()) {
         sum += iter.next().get();
       }
       if (sum >= threshold) {
         double pXY = sum / totalLines;
         PAIR.set((float)Math.log10(pXY/(pX * pY)), sum);
         context.write(key, PAIR);
       }
       
     }
   }
 
   private static final class MyPartitioner extends Partitioner<PairOfStrings, IntWritable> {
     @Override
     public int getPartition(PairOfStrings key, IntWritable value, int numReduceTasks) {
       return (key.getLeftElement().hashCode() & Integer.MAX_VALUE) % numReduceTasks;
     }
   }
 
   /**
    * Creates an instance of this tool.
    */
   private StripesPMI() {}
 
   private static final class Args {
     @Option(name = "-input", metaVar = "[path]", required = true, usage = "input path")
     String input;
 
     @Option(name = "-output", metaVar = "[path]", required = true, usage = "output path")
     String output;
 
     @Option(name = "-reducers", metaVar = "[num]", usage = "number of reducers")
     int numReducers = 1;
 
     @Option(name = "-threshold", metaVar = "[num]", usage = "threshold")
     int threshold = 10;
   }
 
   /**
    * Runs this tool.
    */
   @Override
   public int run(String[] argv) throws Exception {
     final Args args = new Args();
     CmdLineParser parser = new CmdLineParser(args, ParserProperties.defaults().withUsageWidth(100));
 
     try {
       parser.parseArgument(argv);
     } catch (CmdLineException e) {
       System.err.println(e.getMessage());
       parser.printUsage(System.err);
       return -1;
     }
 
     LOG.info("Tool: " + StripesPMI.class.getSimpleName());
     LOG.info(" - input path: " + args.input);
     LOG.info(" - output path: " + args.output);
     LOG.info(" - number of reducers: " + args.numReducers);
     LOG.info(" - threshold: " + args.threshold);
     Configuration conf = getConf();
     Job job = Job.getInstance(conf);
     job.setJobName(StripesPMI.class.getSimpleName());
     job.setJarByClass(StripesPMI.class);
 
     job.setNumReduceTasks(args.numReducers);
 
     FileInputFormat.setInputPaths(job, new Path(args.input));
     FileOutputFormat.setOutputPath(job, new Path("testdir"));
 
     job.setMapOutputKeyClass(Text.class);
     job.setMapOutputValueClass(IntWritable.class);
     job.setOutputKeyClass(Text.class);
     job.setOutputValueClass(IntWritable.class);
     job.setOutputFormatClass(TextOutputFormat.class);
 
     job.setMapperClass(MapperOne.class);
     job.setReducerClass(PairPMIReducer.class);
     job.getConfiguration().setInt("mapred.max.split.size", 1024 * 1024 * 32);
     job.getConfiguration().set("mapreduce.map.memory.mb", "3072");
     job.getConfiguration().set("mapreduce.map.java.opts", "-Xmx3072m");
     job.getConfiguration().set("mapreduce.reduce.memory.mb", "3072");
     job.getConfiguration().set("mapreduce.reduce.java.opts", "-Xmx3072m");
 
 
     // Delete the output directory if it exists already.
     Path outputDir = new Path("testdir");
     FileSystem.get(conf).delete(outputDir, true);
 
     long startTime = System.currentTimeMillis();
     job.waitForCompletion(true);
     LOG.info("Job Finished in " + (System.currentTimeMillis() - startTime) / 1000.0 + " seconds");
     LOG.info("Line count:" + job.getCounters().findCounter("P", "lineNo").getValue());
     /*try{
       FileSystem fs = FileSystem.get(new Configuration());
        FileStatus[] status = fs.listStatus(new Path("testdir"));  // you need to pass in your hdfs path
 
       for (int i=0;i<status.length;i++){
           BufferedReader br=new BufferedReader(new InputStreamReader(fs.open(status[i].getPath())));
           String line;
           line=br.readLine();
           while (line != null){
             System.out.println(line);
             line=br.readLine();
           }
       }
     }catch(Exception e){
         System.out.println("File not found");
     }*/
 
     long c = job.getCounters().findCounter("P", "lineNo").getValue();
     Job job2 = Job.getInstance(conf);
     
     job2.setJobName(StripesPMI.class.getSimpleName());
     job2.setJarByClass(StripesPMI.class);
 
     job2.setNumReduceTasks(args.numReducers);
 
     job2.getConfiguration().setInt("mapred.max.split.size", 1024 * 1024 * 32);
     job2.getConfiguration().set("mapreduce.map.memory.mb", "3072");
     job2.getConfiguration().set("mapreduce.map.java.opts", "-Xmx3072m");
     job2.getConfiguration().set("mapreduce.reduce.memory.mb", "3072");
     job2.getConfiguration().set("mapreduce.reduce.java.opts", "-Xmx3072m");
     FileInputFormat.setInputPaths(job2, new Path(args.input));
     FileOutputFormat.setOutputPath(job2, new Path(args.output));
 
     job2.setMapOutputKeyClass(PairOfStrings.class);
     job2.setMapOutputValueClass(IntWritable.class);
     job2.setOutputKeyClass(PairOfStrings.class);
     job2.setOutputValueClass(DoubleWritable.class);
     job2.setOutputFormatClass(TextOutputFormat.class);
 
     job2.setMapperClass(MapperTwo.class);
     job2.setReducerClass(ReducerTwo.class);
     job2.setPartitionerClass(MyPartitioner.class);
 
     // Delete the output directory if it exists already.
     Path outputDir2 = new Path(args.output);
     FileSystem.get(conf).delete(outputDir2, true);
 
     long startTime2 = System.currentTimeMillis();
     job2.getConfiguration().setDouble("lineCount", c);
     job2.getConfiguration().setInt("threshold", args.threshold);
     job2.waitForCompletion(true);
     LOG.info("Job2 Finished in " + (System.currentTimeMillis() - startTime2) / 1000.0 + " seconds");
     return 0;
   }
 
   /**
    * Dispatches command-line arguments to the tool via the {@code ToolRunner}.
    *
    * @param args command-line arguments
    * @throws Exception if tool encounters an exception
    */
   public static void main(String[] args) throws Exception {
     ToolRunner.run(new StripesPMI(), args);
   }
 }
 